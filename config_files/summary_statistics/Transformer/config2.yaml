prng_key: 13414
loss_config:
  p : 1
  nr_acf_lags: 30
  use_acf_directly: True
learn_config:
  learn_acf: True
  learn_marginal: False
  learn_both: False
trawl_config:
  tau: 1.0
  seq_len: 1500
  theta_size: 5
  batch_size: 32
  marginal_distr: NIG
  acf: sup_IG
  trawl_type: sup_ig_nig_5p
  acf_prior_hyperparams:
    distr_name: uniform
    gamma_prior_hyperparams: [10.,20.]
    eta_prior_hyperparams: [10.,20.]   
  marginal_distr_hyperparams:
    distr_name: uniform 
    loc_prior_hyperparams: [-1.,1.]
    scale_prior_hyperparams: [0.5,1.5]
    beta_prior_hyperparams: [-5.,5.] 
model_config:
    model_name: TimeSeriesTransformerBase
    hidden_size: 64
    num_heads: 2
    num_layers: 2
    mlp_dim: 32
    linear_layer_sizes: [16, 8, 4]  # Now configurable like LSTM
    dropout_rate: 0.05
    final_output_size: 2  # For your two ACF parameters
    freq_attention: true  # Set to false to use regular attention
    with_theta: False
    experiment_dir: summary_statistics
train_config:
  n_iterations: 2501 
val_config:
  val_freq: 250
  val_n_batches: 25
optimizer:
    name: adam
    lr: 0.0005
    weight_decay: 0.0005