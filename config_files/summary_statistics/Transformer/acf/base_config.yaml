prng_key: 243
loss_config:
  p : 2
  nr_acf_lags: 35
  use_acf_directly: True
  num_KL_samples: 5000
  use_kl_div: True
learn_config:
  learn_acf: True
  learn_marginal: False
  learn_both: False
trawl_config:
  tau: 1.0
  seq_len: 1500
  theta_size: 5
  batch_size: 32
  marginal_distr: NIG
  acf: sup_IG
  trawl_process_type: sup_ig_nig_5p
  acf_prior_hyperparams:
    distr_name: uniform
    gamma_prior_hyperparams: [10.,20.]
    eta_prior_hyperparams: [10.,20.]   
  marginal_distr_hyperparams:
    distr_name: uniform 
    loc_prior_hyperparams: [-1.,1.]
    scale_prior_hyperparams: [0.5,1.5]
    beta_prior_hyperparams: [-5.,5.] 
model_config:
    model_name: TimeSeriesTransformerBase
    hidden_size: 24
    num_heads: 2
    num_layers: 2
    mlp_dim: 32
    linear_layer_sizes: [16, 8, 4]  
    dropout_rate: 0.05
    final_output_size: 2  # For your two ACF parameters
    freq_attention: True  # Set to false to use regular attention
    with_theta: False
    experiment_dir: summary_statistics
train_config:
  n_iterations: 25001 
val_config:
  val_freq: 500
  val_n_batches: 45
optimizer:
  name: adam
  lr: 0.0003
  weight_decay: 0.00001